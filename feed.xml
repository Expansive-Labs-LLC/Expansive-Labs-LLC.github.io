<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title></title>
    <subtitle></subtitle>
    <link href="/feed.xml" rel="self" type="application/atom+xml" />
    <link href="" rel="alternate" type="text/html"/>
    <author>
        <name></name>
    </author>
    
    <updated>2025-03-29T00:00:00Z</updated>
    
    <id>/</id>
        <entry>
            <title>Browser Automation and Productivity Pt. 1</title>
            <link href="/posts/browser-automation-pt1/"/>
            <updated>2025-03-29T00:00:00Z</updated>
            <id>/posts/browser-automation-pt1/</id>
            <content type="html"><![CDATA[
                <p><strong>In today’s post, we’re kicking off a three-part series dedicated to taming the everyday chaos of business management through the power of automation. Many of us, myself included, spend countless hours on repetitive tasks that could be handled more efficiently, freeing up valuable time to focus on growth and innovation (or, in my current exciting phase, preparing for a new arrival!).</strong></p>
<p><strong>Part One:</strong> We’re tackling a common pain point: <strong>managing and synchronizing multiple calendars.</strong> If you’ve ever struggled to keep your personal, work, and client schedules aligned, you know how quickly those manual updates can eat into your day. We’ll be diving into how <strong>low-code/no-code automation platforms</strong> like Zapier and Make can be used to create powerful workflows that seamlessly connect your calendars, using my own recent experience with an Airbnb client’s booking schedule as a real-world example.<br />
<strong>Part Two:</strong> We’ll explore how <strong>browser extensions</strong> like Browserflow can streamline repetitive browser-based tasks, using the scenario of <strong>automatically generating invoices for repeat clients.</strong><br />
<strong>Part Three:</strong> For the more technically inclined, we’ll venture into the world of <strong>headless browsers and scripting</strong> to demonstrate how you can implement sophisticated automation, such as <strong>automatically removing events from a calendar when a booking is cancelled.</strong></p>
<h1>Taming the Browser (and Your Calendars!) with Automation</h1>
<p>Between managing my various businesses and preparing for a new chapter in my life as a mom, time has become my most precious commodity. Recently, I found myself spending hours wrestling with scheduling and administrative tasks that felt like they should have a more efficient solution. At Expansive Labs, we’re constantly seeking innovative ways to boost productivity, and one area ripe for optimization is how we interact with the web every single day. Join us as we explore the fascinating world of browser automation and uncover how it can streamline your schedule and ultimately give you back control of your time.</p>
<h2>The Daily Grind: A Browser-Centric Reality</h2>
<p>Think about your typical workday. How much time do you spend within your browser? Probably a significant chunk! You’re likely checking calendars, navigating invoicing software, pulling information, and sending emails. While these tasks are essential, the manual back-and-forth and sheer repetition can be incredibly draining.</p>
<h2>Enter the Realm of Browser Automation</h2>
<p>Browser automation involves using tools to instruct your web browser to perform repetitive tasks automatically. Think of it as having a digital assistant that can navigate websites, fill out forms, click buttons, and extract data based on pre-defined rules. This can significantly streamline tasks like:</p>
<ul>
<li><strong>Automated Schedule Updates:</strong> Keeping multiple online calendars in sync.</li>
<li><strong>Simplified Invoice Generation:</strong> Extracting data and populating invoices automatically.</li>
<li><strong>Data Entry:</strong> Automating the transfer of information between online platforms.</li>
</ul>
<h2>Real-World Automation: Syncing Your Airbnb Calendar with Zapier</h2>
<p>Recently, navigating my own schedule became particularly challenging due to managing an Airbnb client’s booking calendar alongside my other commitments. I needed a way to automatically reflect “unavailable” slots from the Airbnb calendar in my general work calendar <em>without</em> sharing the detailed booking information.</p>
<p>This led me to explore the power of integration platforms like <a href="https://zapier.com">Zapier</a> and <a href="https://www.make.com/en/integrations">Make</a>. These platforms allow you to create automated workflows between different applications.</p>
<p>Let’s walk through a specific example of how to automate this using Zapier:</p>
<p><strong>Goal:</strong> When a new reservation appears in your “Airbnb Client’s Calendar” (which you have view access to), create a “Not available**air b and b” event from 12:30 PM to 1:15 PM on the first day of that reservation in your “Work Availability” Google Calendar.</p>
<p><strong>Here’s a step-by-step approach to setting up this Zap:</strong></p>
<ol>
<li>
<p><strong>Trigger: New Event in Google Calendar</strong></p>
<ul>
<li><strong>Choose the App:</strong> Select “Google Calendar”.</li>
<li><strong>Choose the Trigger Event:</strong> Select “New Event”.</li>
<li><strong>Connect Account:</strong> Connect your Google account with access to the “Airbnb Client’s Calendar”.</li>
<li><strong>Choose Calendar:</strong> Select the “Airbnb Client’s Calendar”.</li>
<li><strong>Test Trigger:</strong> Zapier will find a recent event to ensure the connection works.</li>
</ul>
</li>
<li>
<p><strong>Action: Format Date/Time (Isolate the First Day)</strong></p>
<ul>
<li><strong>Choose the App:</strong> Select “Formatter by Zapier”.</li>
<li><strong>Choose the Action Event:</strong> Select “Date / Time”.</li>
<li><strong>Transform:</strong> Choose “Format”.</li>
<li><strong>Input:</strong> Select the “Start Time” field from the “New Event in Google Calendar” trigger.</li>
<li><strong>To Format:</strong> Choose “YYYY-MM-DD”.</li>
<li><strong>Output Timezone:</strong> Set your timezone (America/Detroit).</li>
<li><strong>Include Time in Output:</strong> Select “No”.</li>
<li><strong>Test Action:</strong> This will output the start date of the Airbnb reservation.</li>
</ul>
</li>
<li>
<p><strong>Action: Create Detailed Event in Your “Work Availability” Google Calendar</strong></p>
<ul>
<li><strong>Choose the App:</strong> Select “Google Calendar”.</li>
<li><strong>Choose the Action Event:</strong> Select “Create Detailed Event”.</li>
<li><strong>Connect Account:</strong> Connect your Google account managing your “Work Availability” calendar.</li>
<li><strong>Choose Calendar:</strong> Select your “Work Availability” Google Calendar.</li>
<li><strong>Summary:</strong> Enter “Not available**air b and b”.</li>
<li><strong>Start Date:</strong> Use the output from the “Format Date/Time” action.</li>
<li><strong>Start Time:</strong> Enter “12:30”.</li>
<li><strong>End Date:</strong> Use the output from the “Format Date/Time” action.</li>
<li><strong>End Time:</strong> Enter “13:15”.</li>
<li><strong>Timezone:</strong> Set your timezone (America/Detroit).</li>
<li><strong>(Optional) Description:</strong> Add “Blocked for Airbnb reservation.”</li>
<li><strong>(Optional) Visibility:</strong> Set as needed.</li>
<li><strong>Test Action:</strong> Zapier will attempt to create a test event.</li>
</ul>
</li>
</ol>
<p><strong>Important Considerations:</strong></p>
<ul>
<li><strong>Trigger Specificity:</strong> Ensure the trigger accurately identifies new reservations.</li>
<li><strong>Recurring Bookings:</strong> This setup assumes each day of a booking is a separate event. More complex workflows are needed for multi-day single events.</li>
<li><strong>Time Zone Consistency:</strong> Verify time zones are correctly configured.</li>
<li><strong>Error Handling:</strong> Consider setting up notifications for Zap failures.</li>
<li><strong>Thorough Testing:</strong> Always test your Zaps after creation.</li>
</ul>
<h2>Getting Started with Automation</h2>
<p>The thought of automating tasks might seem daunting, but the time saved and reduced errors are invaluable. Start by identifying your most repetitive tasks and exploring tools like Zapier and Make.</p>
<h2>The Expansive Potential</h2>
<p>From calendar management to invoicing and beyond, automation offers significant opportunities to reclaim your time. Stay tuned for Parts Two and Three where we’ll explore browser extensions and headless browsers for even more powerful automation!</p>
<p><strong>#BrowserAutomation #Productivity #AutomationTools #BusinessEfficiency #TimeManagement #CalendarSync #NewMom #ExpansiveLabs</strong></p>

            ]]></content>
        </entry>
        <entry>
            <title>Deep Dive Into LM Studio</title>
            <link href="/posts/lm-studio/"/>
            <updated>2025-03-27T00:00:00Z</updated>
            <id>/posts/lm-studio/</id>
            <content type="html"><![CDATA[
                <h2>Level Up Your Open Source Contributions: A Deep Dive into LM Studio</h2>
<p>As someone deeply involved in developing and nurturing open-source communities, I’m always looking for tools that boost my productivity and help me engage more effectively. Recently, I’ve discovered a game-changer: <strong>LM Studio</strong>.  It’s allowed me to run powerful Large Language Models (LLMs) <em>locally</em> on my machine, opening up exciting possibilities for everything from code generation and documentation to community moderation and content creation. In this post, I’ll walk you through setting up LM Studio and how it’s become an integral part of my workflow.</p>
<h3>Why Run LLMs Locally? The Open Source Advantage</h3>
<p>You might be wondering why bother with local LLMs when there are so many cloud-based options available. For me, the benefits are significant, especially within the open-source context:</p>
<ul>
<li><strong>Privacy:</strong>  Keeping everything on my machine means no data leaves my control – crucial when dealing with potentially sensitive project details or community discussions.</li>
<li><strong>Cost:</strong> No API costs! Once you’ve downloaded the models, usage is free. [Mention if you were spending a significant amount on API calls before]</li>
<li><strong>Offline Access:</strong>  I can continue working even without an internet connection – perfect for travel or unreliable connectivity.</li>
<li><strong>Customization &amp; Experimentation:</strong> Local LLMs give me more freedom to experiment with different models and parameters without restrictions.</li>
<li><strong>Supporting the Open Source Ecosystem</strong>: By running these models locally, you’re directly supporting the developers who created them!</li>
</ul>
<h3>Getting Started with LM Studio: A Step-by-Step Guide</h3>
<p>Setting up LM Studio is surprisingly straightforward. Here’s a breakdown:</p>
<ol>
<li><strong>Download &amp; Installation:</strong> Head over to <a href="https://lmstudio.ai/">https://lmstudio.ai/</a> and download the version for your operating system (Windows, macOS, Linux). The installation process is simple – just follow the on-screen instructions.</li>
<li><strong>The LM Studio Interface:</strong> When you launch LM Studio, you’ll be greeted with a clean and intuitive interface. It’s divided into three main sections:
<ul>
<li><strong>Home:</strong> Where you can discover and download models.</li>
<li><strong>Chat:</strong> The primary interface for interacting with your chosen model.</li>
<li><strong>Local Server:</strong>  Allows you to run LM Studio as an API server, which is <em>extremely</em> useful (more on that later!).</li>
</ul>
</li>
<li><strong>Downloading Your First Model:</strong> This is where the fun begins! Click on the “Home” tab and use the search bar to find models. You can filter by size, license, and other criteria. For this guide, we’ll focus on three excellent options: Dolphin, Deepseek, and Gemma (more details below).  Click the download button next to your chosen model. LM Studio will handle the rest – it automatically downloads the necessary files and prepares the model for use.</li>
<li><strong>Choosing Quantization</strong>: When downloading a model you’ll be asked about quantization. Lower quantizations (like Q4_K_M) are smaller and faster, but may sacrifice some quality. Higher quantizations (Q8_0) offer better quality at the cost of size and speed. Experiment to find what works best for your hardware!</li>
</ol>
<h3>My Go-To Models: Dolphin, Deepseek &amp; Gemma</h3>
<p>I’ve been experimenting with several models in LM Studio, but these three have consistently delivered impressive results for my open-source work:</p>
<ul>
<li><strong>Dolphin:</strong> Known for its strong conversational abilities and helpfulness. I find it excellent for brainstorming ideas, drafting documentation, and even role-playing different user personas to anticipate community questions. [Mention a specific example of how Dolphin helped you with this]</li>
<li><strong>Deepseek:</strong>  A powerful model that excels at code generation and understanding complex technical concepts. It’s been invaluable for helping me refactor existing code, write unit tests, and even explore new architectural approaches.</li>
<li><strong>Gemma:</strong> Google’s open-weight model is a great all-rounder. I use it for tasks like summarizing long documents (like RFCs or design proposals) and generating creative content for social media announcements.</li>
</ul>
<p>Each of these models has its strengths, so I often switch between them depending on the task at hand.  LM Studio makes this incredibly easy – you can have multiple models downloaded and ready to go.</p>
<h3>How LM Studio Powers My Open Source Workflow: Specific Use Cases</h3>
<p>Here’s how I’m leveraging LM Studio in my day-to-day open-source activities:</p>
<ul>
<li><strong>Documentation Generation &amp; Improvement:</strong>  Writing clear, concise documentation is crucial for any successful open-source project. I use Dolphin and Gemma to help me draft initial documentation outlines, rewrite existing sections for clarity, and even translate documentation into different languages.</li>
<li><strong>Code Assistance &amp; Refactoring (Deepseek):</strong> Deepseek has been a lifesaver when tackling complex coding tasks.  I can paste code snippets directly into the chat interface and ask it to explain what they do, identify potential bugs, or suggest improvements. It’s also great for generating boilerplate code for new features.</li>
<li><strong>Community Support &amp; Moderation:</strong> I’m experimenting with using LM Studio (via the Local Server API – see below) to help me summarize long forum threads or Slack conversations, identifying key issues and sentiment. This could eventually lead to automated tools that assist community moderators. [Explain if you’re actively building something like this]</li>
<li><strong>Content Creation for Social Media &amp; Blog Posts:</strong>  Generating engaging content is essential for attracting contributors and raising awareness about the project. I use Gemma to help me brainstorm ideas, write compelling social media posts, and even draft blog post outlines (like this one!).</li>
<li><strong>Issue Triage</strong>: Pasting issue descriptions into LM Studio can quickly summarize the problem and suggest potential solutions or areas to investigate.</li>
</ul>
<h3>Unleashing the Power of the Local Server API</h3>
<p>One of the most powerful features of LM Studio is its ability to run as a local API server. This allows you to integrate LLMs into your existing tools and workflows.  Here’s how I’m using it:</p>
<ol>
<li><strong>Start the Server:</strong> In LM Studio, navigate to the “Local Server” tab and click “Start Server.”</li>
<li><strong>Access the API:</strong> LM Studio will provide an endpoint (usually <code>http://localhost:1234/v1</code>).</li>
<li><strong>Integrate with Your Tools</strong>: You can then use standard HTTP requests or a library like <code>requests</code> in Python to send prompts to the server and receive responses.</li>
</ol>
<p>This opens up endless possibilities for automation and integration.  I’m currently exploring ways to integrate the API into my project’s CI/CD pipeline to automatically generate release notes and documentation updates. [Share any specific code snippets or examples if you’re comfortable]</p>
<h3>Resources &amp; Further Exploration</h3>
<ul>
<li><strong>LM Studio:</strong> <a href="https://lmstudio.ai/">https://lmstudio.ai/</a></li>
<li><strong>Hugging Face Hub:</strong> <a href="https://huggingface.co/">https://huggingface.co/</a> (for discovering more models)</li>
<li><strong>TheBloke’s Models</strong>: <a href="https://huggingface.co/TheBloke">https://huggingface.co/TheBloke</a> - A great source for quantized models optimized for LM Studio.</li>
</ul>
<h3>Conclusion</h3>
<p>LM Studio has been a game-changer for my open-source work. By bringing the power of LLMs to my local machine, I’ve gained increased privacy, cost savings, and flexibility.  If you’re involved in open-source development or community management, I highly recommend giving it a try. It might just unlock new levels of productivity and creativity for you too!</p>

            ]]></content>
        </entry>
        <entry>
            <title>Load Balancing in the Cloud</title>
            <link href="/posts/load-balancing-in-the-cloud/"/>
            <updated>2018-06-05T00:00:00Z</updated>
            <id>/posts/load-balancing-in-the-cloud/</id>
            <content type="html"><![CDATA[
                <p>Free Download: <a href="https://www.nginx.com/resources/library/load-balancing-in-the-cloud-aws-nlb-elb-nginx-plus/">Load Balancing in the Cloud, Practical Solutions with NGINX and AWS</a></p>
<h1>About the Book</h1>
<p>The use of redundant servers has long been a solution for meeting sudden spikes in demand, machine failures, and outages. Cloud services greatly reduce the cost and hassle of provisioning redundant equipment and load balancers and give you the ability to deal with separate network, application, and client-side loads. But today there are many options to consider, and you have to determine which cloud service (or services) will meet your unique needs.</p>
<p>With this ebook, I deliver a practical guide to load balancing services in the cloud. In this dynamic environment, where machines are frequently provisioned and decommissioned to meet user demand, you need a load balancer that can intelligently distribute traffic. This ebook walks you through the strategy via several options, including the use of the NGINX software load balancer with AWS Network Load Balancer (AWS NLB) and global load balancing with AWS’ Route 53 offering.</p>
<h1>Preface</h1>
<p>This book is for engineers and technical managers looking to take advantage of the cloud in a way that requires a load balancing solution. I am using AWS as the example because it is widely used, and therefore will be the most useful to the most people. You’ll learn about load balancing in general, as well as AWS load balancers, AWS patterns, and the NGINX reverse proxy and load balancer. I’ve chosen to use NGINX as a software load balancer example because of its versatility and growing popularity. As adoption of NGINX grows, there are more people looking to learn about different ways they can apply the technology in their solutions. My goal is to help educate you on how you can craft a load balancing solution in the cloud that fits your needs without being prescriptive, but rather descriptive and informative.</p>
<p>I wrote this text to complement the AWS Quick Start guide to NGINX Plus. I truly believe in NGINX as a capable application delivery platform, and AWS as an industry leading cloud platform. That being said, there are other solutions to choose from, such as: Google Cloud, Microsoft Azure, Digital Ocean, IBM Cloud, their respective platform native load balancers, HAProxy, the Apache HTTP Server with the mod_proxy module, and IIS with the URL Rewrite module. As a cloud consultant, I understand that each cloud application has different load balancing needs. I hope that the information in this book helps you design a solid solution that fits your performance, security, and availability needs, while being economically reasonable.</p>
<p>As you read through keep your application architecture in mind. Compare and contrast the feature set you might need with the up-front and ongoing cost of building and managing the solution. Pay special attention to the automatic registration and deregistration of nodes with the load balancer. Even if you do not plan to auto scale today, it is wise to prepare with a load balancing solution that is capable of doing so to enable your future.</p>

            ]]></content>
        </entry>
        <entry>
            <title>AWS Batch</title>
            <link href="/posts/aws-batch/"/>
            <updated>2018-04-13T00:00:00Z</updated>
            <id>/posts/aws-batch/</id>
            <content type="html"><![CDATA[
                <h1>AWS Batch Example</h1>
<p>Find the code on <a href="https://github.com/dejonghe/aws-batch-example">GitHub</a></p>
<h2>Objective</h2>
<p>This is example of the Amazon Batch Service to deploy an example batch workflow. This example is meant to be a deployable artifact POC. This example will use CloudFormation, AWS Lambda, Docker, AWS Batch, and an S3 bucket trigger.</p>
<p>The batch workflow created in this example is not a prescription for how batch processing should be done but merely an example. In this example all of the jobs for the workflow are scheduled at once. In doing so your scheduler could pass environment variables or change the command of each job to instruct the jobs where to store data, and where to expect data. Another approach would be to, have processing jobs schedule their dependent jobs, instructing those dependent jobs where to find the data that they produced. There are many ways batch processing can be utilized, this is simply an example.</p>
<h3>Why I Think Batch is Cool</h3>
<ul>
<li>AWS Batch can manage the infrastructure for you, scaling up or down based on the number of jobs in queue.</li>
<li>AWS Batch is able to scale vertically as well, when your Compute Environment Instance Type is set to “optimal”.</li>
<li>AWS Batch can automatically bid on spot instances for you.</li>
<li>Your Job queues can be worked by multiple Compute Environments.
<ul>
<li>You can set preference for a Spot Instance Compute Environment</li>
<li>When your first Compute Environment is unavailable or over worked the job can be picked up by the next.</li>
</ul>
</li>
<li>Job Queues have priority, which means you can have high priority jobs completed first.</li>
<li>Job Definitions can be either docker tasks or Lambda Functions.</li>
<li>Job Definitions can have default environments, mount points, volumes, CMD, etc.</li>
<li>Job Definition defaults can be overrode on Submission</li>
<li>Jobs can have dependencies</li>
<li>You can run an array of the same job, making Monte Carlo simulations a breeze.</li>
</ul>
<hr />
<h2><strong>Note: You are responsible for the costs incurred by running this example.</strong></h2>
<p>The example is limited to a total of 24 CPU’s at a given time unless altered, and defaults to 0 running instances when no jobs are in queue.</p>
<hr />
<h2>Diagram</h2>
<p><img src="/static/images/Batch-POC-Diagram.png" alt="Diagram" /></p>
<h2>Preparation and Demonstration</h2>
<p>Before you jump head first into AWS Batch you need code for batch to run. AWS batch is able to run docker container tasks, and AWS Lambda Functions. This example will use docker container tasks, and a Lambda function to submit a job when an object is created or modified in a S3 bucket. This repository consists of everything you will need to build the proof of concept. There is a prep script that will build the Lambda code into a zip package and upload it to a S3 Bucket. This script will then validate you have ECR repositories available to host the docker container images that are used, fetch credentials for the ECR repositories, then build and push the images. After this you will deploy the CloudFormation to produce the AWS Batch POC. This documentation will walk you through running all of the commands and explain what they do.</p>
<h3>Assumptions:</h3>
<ul>
<li>You have an AWS Account.</li>
<li>You’re using Bash.</li>
<li>You have pip installed. <a href="https://pip.pypa.io/en/stable/installing/">Help</a></li>
<li>You have the AWS CLI installed, preferred version 1.11.108 or greater. <a href="https://docs.aws.amazon.com/cli/latest/userguide/installing.html">Help</a></li>
<li>You have configured the CLI, set up AWS IAM access credentials that have appropreate access. <a href="https://docs.aws.amazon.com/cli/latest/reference/configure/index.html">Help</a></li>
<li>You have docker installed. <a href="https://docs.docker.com/install/">Help</a></li>
<li>You have a Ec2 KeyPair in your AWS Account / Default Region <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html">Help</a></li>
<li>You have a VPC with at least two subnets that have internet access, you can use the default VPC. <a href="https://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/default-vpc.html">Help</a></li>
</ul>
<h3>Step 1: Create a S3 Bucket.</h3>
<p>You will need a S3 bucket to work out of, we will use this bucket to upload our lambda code zip. Create the bucket with the following CLI command or through the console. Keep in mind that S3 bucket names are globally unique and you will have to come up with a bucket name for yourself.</p>
<pre><code>aws s3 mb s3://aws_batch_example_{yourName} (--profile optionalProfile)
</code></pre>
<h3>Step 2: Clone the example Github project.</h3>
<p>I have prepared a Github project with all of the example CloudFormation and code to get you off the ground. Clone this Github project to your local machine.</p>
<p><a href="https://github.com/dejonghe/aws-batch-example">https://github.com/dejonghe/aws-batch-example</a></p>
<h3>Step 3: Run the env_prep Script.</h3>
<p>To prepare</p>
<p>You must run a script from within the Github project. This script is to be ran from the base of the repository. If you rename the repository directory you will need to edit the <a href="https://github.com/dejonghe/aws-batch-example/scripts/env_prep.sh">script</a>, all the variables are at the top.</p>
<p>This script performs the following tasks:</p>
<ol>
<li>Builds and and uploads the lambda code
<ul>
<li>The script creates a temp directory</li>
<li>Copies the code from <a href="https://github.com/dejonghe/aws-batch-example/lambda/trigger/">lambda trigger</a> to the temp directory</li>
<li>Uses pip to install the requirements to the temp directory</li>
<li>Zips up the contents of the temp directory to a package named ./lambda/trigger.zip</li>
<li>Removes the temp directory</li>
<li>Uploads the zip to <code>s3://{yourBucket}/{release(develop)}/lambda/trigger.zip</code></li>
</ul>
</li>
<li>Creates and or Validates ECR Repositories
<ul>
<li>The script will validate you have two ECR Repositories: [‘batchpocstart’,‘batchpocprocess’]</li>
<li>If either of the repositories are missing it will create them.</li>
</ul>
</li>
<li>Get docker login credentials for the repositories
<ul>
<li>The script uses <code>aws ecr get-login</code> piped to <code>bash</code></li>
</ul>
</li>
<li>Docker images are built and pushed
<ul>
<li>The script moves into the directory containing the Dockerfiles</li>
<li><a href="https://github.com/dejonghe/aws-batch-example/docker/batchpocstart/">batchpocstart</a>: Has a small python application that schedules jobs that depend on each other.</li>
<li><a href="https://github.com/dejonghe/aws-batch-example/docker/batchpocprocess/">batchpocprocess</a>: Is just a busybox container that runs <code>echo &quot;hello world&quot;</code> to emulate work.</li>
<li>Once in the given directory the script will use the docker command line tool to build the image.</li>
<li>The images are tagged to match the repository created previously. {accountId}.dkr.ecr.{region}.amazonaws.com/{repoName}:latest</li>
<li>Images are pushed to ECR.</li>
</ul>
</li>
</ol>
<p>The following is an example of running the script. <strong>Note:</strong> You can pass -p profile, and -r release (Your aws-cli profile’s default regions is used)</p>
<pre><code>./scripts/env_prep.sh -b aws_batch_example_{yourName} (-p optionalProfile -r optionalRelease)
</code></pre>
<h3>Step 4: Create the CloudFormation Stack.</h3>
<p>This step utilizes the <a href="https://github.com/dejonghe/aws-batch-example/cloudformation/batch/batch-example.yml">CloudFormation Template</a> to produce the batch environment and supporting resources and hooks. You can upload this template directly to the AWS CloudFormation Console, or use the following command. The console will provide drop downs for the networking and Ec2 KeyPair parameters. If you use the command line you’ll have to look these up yourself to fill out the command line arguments. If you want SSH access to the batch instances, ensure that the VPCCidr parameter is set to the Cidr of your VPC, it’s used for the batch instance security group. You must provide IAM capabilities to this CloudFormation stack because we must create an IAM role for the lambda function, the batch instances, and the batch containers. A full description of the CloudFormation stack and what it creates can be found in the <a href="https://github.com/dejonghe/aws-batch-example/cloudformation/batch/README.md">README.md</a></p>
<pre><code>aws cloudformation create-stack --template-body file://cloudformation/batch/batch-example.yml --stack-name batch-example --parameters '[{&quot;ParameterKey&quot;:&quot;CloudToolsBucket&quot;,&quot;ParameterValue&quot;:&quot;aws_batch_example_{yourName}&quot;},{&quot;ParameterKey&quot;:&quot;VPCCidr&quot;,&quot;ParameterValue&quot;:&quot;10.0.0.0/16&quot;},{&quot;ParameterKey&quot;:&quot;VPC&quot;,&quot;ParameterValue&quot;:&quot;vpc-xxxxxxxx&quot;},{&quot;ParameterKey&quot;:&quot;Subnets&quot;,&quot;ParameterValue&quot;:&quot;subnet-xxxxxxxx,subnet-xxxxxxxx&quot;},{&quot;ParameterKey&quot;:&quot;Ec2KeyPair&quot;,&quot;ParameterValue&quot;:&quot;{yourKeyPairName}&quot;}]' --capabilities CAPABILITY_NAMED_IAM (--profile optionalProfile)

</code></pre>
<p>Wait for the CloudFormation stack to complete and then check in on the Batch Dashboard.</p>
<h3>Step 6: Exercise the demo</h3>
<h4>Upload to S3</h4>
<p>The CloudFormation stack created a S3 bucket. When a object is created or updated in this bucket, a notification triggers the lambda to submit a job to AWS batch. The S3 bucket is named: <code>batch-poc-{yourAccountId}-{region}</code>. Upload a file to this bucket through the console or with the following command and watch a job get submitted to AWS Batch.</p>
<pre><code>aws s3 cp ./README.md s3://batch-poc-{yourAccountId}-{region}/ (--profile optionalProfile)
</code></pre>
<h4>Watch the Batch Dashboard</h4>
<p>From the Batch Dashboard you should see the Queue, and Compute Environment. You will be able to track the status of jobs from the Dashboard. <strong>Make sure to refresh</strong></p>
<ol>
<li>You’ll see a job appear in the submitted state.</li>
<li>It will quickly jump to Pending then to Runnable.</li>
<li>Once the job is runnable there will need to be a instance in the Compute Environment to run the job.</li>
<li>Note that the current Desired vCPUs for your Compute Environment is initially 0. This means you have 0 instance in your Compute Environment.</li>
<li>As the job becomes runnable Batch will start to launch an EC2 instance for the job to run on. This can take a few minutes.
<ul>
<li>To keep a Batch Compute Environment warm so you do not have to wait, up the Minimum vCPUs.</li>
</ul>
</li>
<li>Once your Compute Environment is online you will see your job move to starting.
<ul>
<li>This can take a few moments for the first run because the instance needs to pull down the container image.</li>
</ul>
</li>
<li>Your Job will then move to running for a brief second, and then to Success or Failed.</li>
<li>Whether you job is a Success or Failure, if you click into the link you will be taken to the Jobs Dashboard for that Status.</li>
<li>Click the JobId. You will see detail appear on the right hand side.</li>
<li>In the detail you will see link to view logs. Click on it to explore.</li>
<li>If the first job was successful, 5 other jobs will be submitted and move through the different status’s.
<ul>
<li>Note that when the group of 5 is submitted some stay in pending, and do not immediately go to runnable. This is because those jobs have dependencies.</li>
</ul>
</li>
<li>Now submit a job manually through the Jobs Dashboard.
<ul>
<li>Play with running jobs different ways.</li>
<li>Try submitting an Array.</li>
</ul>
</li>
<li>Find other ways to trigger batch jobs:
<ul>
<li>The lambda function allows you to integrate with anything that can invoke lambda or SNS, though you’ll have to edit the permissions.</li>
<li>Check out the CloudWatch Events trigger for AWS Batch, to trigger based on CloudWatch Event rules/patterns or a time schedule.</li>
</ul>
</li>
</ol>

            ]]></content>
        </entry>
        <entry>
            <title>cfn-boto-interface</title>
            <link href="/posts/cfn-boto-interface/"/>
            <updated>2018-04-12T00:00:00Z</updated>
            <id>/posts/cfn-boto-interface/</id>
            <content type="html"><![CDATA[
                <h1><a href="https://github.com/dejonghe/cfn-boto-interface">cfn-boto-interface</a></h1>
<h2>Note: This should no longer be used, in favor of AWS CDK.</h2>
<h2>What is it?</h2>
<p>Possibly the most dangerous thing I’ve ever written.</p>
<p>This is python3.6 code that is intended to run inside of AWS Lambda. The Lambda Function is intended to be used as a CloudFormation Custom Resource. This Custom Resource takes properties that describe a set of commands to run. It allows you to look up data from the event source, response objects returned by other commands, modify lookups to cast to <code>int</code> or <code>str</code>, and also interpolate random strings.</p>
<h2>Where is it?</h2>
<p>GitHub: <a href="https://github.com/dejonghe/cfn-boto-interface">https://github.com/dejonghe/cfn-boto-interface</a></p>
<h2>Why I wrote this</h2>
<p>I often find myself writing custom resources for things that either are not yet supported by CloudFormation, or for things that seem like they will never be supported, like a lookup. Often these custom resources are just a few boto3 calls that create, update, and delete a given resource. I has started to look at spot fleets for running ECS clusters after the pricing model changed in March 2018. The EC2 Launch Template was not supported at that time and neither was the ability to use EC2 Launch Templates with a Spot Fleet built by CloudFormation. With that challenge I figured I had a couple custom resources to build and I thought about how I could abstract that as much as possible, and I think I found it. I decided to build a direct interface to boto3 through CloudFormation.</p>
<p>Overall, I wrote this to aid in things such as quick lookups or utilizing new features and services before they get CloudFormation Support. It’s already working out for me. I tested it on Secrets Manager with in a week of the service being available and I’m able to retrieve secrets.</p>
<h2>How is it used?</h2>
<p><a href="https://github.com/dejonghe/cfn-boto-interface">The GitHub Repo</a> has more of a full rundown that includes building the zip, creating the Lambda Resource in CloudFormation, etc. You can use the pre built release package directly, as it’s a Zip file containing the code, cfn-response, and boto3==1.7.4. Here I’d just like to add an example and the high level documentation of what this object is made up of.</p>
<h3>Example: Secret String fom Secret Manager</h3>
<p>Note: Use GetAtt for secrets so that it doesn’t show up in CFN console as PhysicalResourceId</p>
<pre><code class="language-yaml">  SecretString:
    Type: Custom::FetchSecret
    Properties:
      # Reference to the Lambda Function that gets called
      ServiceToken: !GetAtt 'BotoInterface.Arn'
      # When a create event type is send to the lambda use this object
      Create:
        PhysicalResourceId: '!Create[0].VersionId' # Lookup and return secret when Ref'd
        ResponseData:                   # Key,Value pairs for GetAtt to use
          Secret: '!Create[0].SecretString'
        Commands:
          - Client: secretsmanager      # Boto3 client to use
            Method: get_secret_value    # method to call on the boto3.client('secretsmanager') object
            Arguments:                  # Keyword Arguments to pass to the method
              SecretId: !Ref 'SecretId'
      Update:
        Replace: 'True'                 # If an update is called just run create again,
                                        # CFN will send a delete on cleanup
</code></pre>
<h3>Custom Resource Properties</h3>
<h4>Top level properties</h4>
<ul>
<li>ServiceToken: (Required) - Arn of the Lambda Resource</li>
<li>Create: ActionObject, described below</li>
<li>Update: ActionObject, described below</li>
<li>Delete: ActionObject, described below</li>
</ul>
<h4>ActionObject</h4>
<ul>
<li>PhysicalResourceId: Physical Id of this resource to return to CloudFormation for this action. Can use <em>Lookups</em></li>
<li>ResponseData: Dict of key,value pairs to return to CloudFormation for this resource for this action, for use in GetAtt. Can use <em>Lookups</em></li>
<li>Commands: Array of CommandObjects</li>
<li>Replace: (Update Only) - Bool, will re run create, if a different PhysicalId is returned CloudFormation will send a Delete when Cleaning Up</li>
</ul>
<h4>CommandObjects</h4>
<ul>
<li>Client: (Required) - Boto3 client name to use when creating a client example: ‘ec2’, or ‘secretsmanager’</li>
<li>Method: (Required) - Method to call on the Boto3 client</li>
<li>Arguments: Dict of key,value pairs to pass to the method as keyword arguments.</li>
</ul>
<h4>Lookups</h4>
<p>Lookups are denoted with a <code>!</code> prefix. The lookups traverse dict objects by use of <code>.</code> notation</p>
<ul>
<li><code>!event</code>: Looks up a value in the event passed to the lambda from CloudFormation</li>
<li><code>!Create[]</code>: (Create ActionObject Only) - Looks up a value from the return of the command at that index ran in the Create ActionObject</li>
<li><code>!Update[]</code>: (Update ActionObject Only) - Looks up a value from the return of the command at that index ran in the Update ActionObject</li>
<li><code>!Delete[]</code>: (Delete ActionObject Only) - Looks up a value from the return of the command at that index ran in the Delete ActionObject</li>
</ul>
<h5>Modifiers</h5>
<p>If a lookup returns a value in a type that you need to cast you can use the modifiers after the lookup notation.</p>
<ul>
<li><code>!int</code>: Cast lookup to int</li>
<li><code>!str</code>: Cast lookup to str</li>
</ul>
<h4>Interpolation</h4>
<ul>
<li><code>!random</code>: Interpolates a random 4 Alpha Numeric string</li>
</ul>
<h2>Conclusion</h2>
<p>I was serious when I said this was dangerous, but then again it’s kind of cool. I hope others find use in it. If you have issues comment here or make an issue in GitHub. I’ll try to help out when I can.</p>

            ]]></content>
        </entry>
        <entry>
            <title>The NGINX Cookbook</title>
            <link href="/posts/nginx-cookbook/"/>
            <updated>2018-03-16T00:00:00Z</updated>
            <id>/posts/nginx-cookbook/</id>
            <content type="html"><![CDATA[
                <h2><a href="https://www.nginx.com/resources/library/complete-nginx-cookbook/">The NGINX Cookbook</a></h2>
<p>In 2016 I started writing for O’REILLY Media to produce The NGINX Cookbook. This is book was organized and released in three parts. The first part focued on load balancing on the modern web. The second, security and access. Lastly, a part about deployment and operations. This blog is not much more than a reference to the book. You can find the book for free at the link above.</p>
<p>Over the years the book has been updated and we’re currently on the 3rd edition.</p>

            ]]></content>
        </entry></feed>